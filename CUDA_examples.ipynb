{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install CUDA C++ plugin for Colab:\n",
        "!pip install nvcc4jupyter\n",
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntOgKF_-y0SV",
        "outputId": "e60ae812-179a-493b-90ea-30d7832896f7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n",
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmp4t1ziot0\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect selected GPU and its NVIDA architecture:\n",
        "import subprocess\n",
        "gpu_info = subprocess.getoutput(\"nvidia-smi --query-gpu=name,compute_cap --format=csv,noheader,nounits\")\n",
        "if \"not found\" in gpu_info.lower(): raise RuntimeError(\"Error: No GPU found. Please select a GPU runtime environment.\")\n",
        "gpu_name, compute_cap = map(str.strip, gpu_info.split(','))\n",
        "gpu_arch = f\"sm_{compute_cap.replace('.', '')}\"\n",
        "\n",
        "print(f\"{'GPU Name':<15}: {gpu_name}\")\n",
        "print(f\"{'Architecture':<15}: {gpu_arch}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVqh0baiO15d",
        "outputId": "8f3eb0b1-e1ed-4f37-dbfa-f7c3505ed46c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Name       : Tesla T4\n",
            "Architecture   : sm_75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda -c \"--gpu-architecture $gpu_arch\"\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void hello_kernel() {\n",
        "    int blockId = blockIdx.x;\n",
        "    int threadId = threadIdx.x;\n",
        "    int globalId = threadId + blockId * blockDim.x;\n",
        "\n",
        "    printf(\"Hello from block %d, thread %d (global thread %d)\\n\", blockId, threadId, globalId);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    int numBlocks = 2;\n",
        "    int threadsPerBlock = 4;\n",
        "\n",
        "    hello_kernel<<<numBlocks, threadsPerBlock>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HW8N8RXzCTc",
        "outputId": "51d804b5-3969-4753-d090-8395f0dc89a3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello from block 0, thread 0 (global thread 0)\n",
            "Hello from block 0, thread 1 (global thread 1)\n",
            "Hello from block 0, thread 2 (global thread 2)\n",
            "Hello from block 0, thread 3 (global thread 3)\n",
            "Hello from block 1, thread 0 (global thread 4)\n",
            "Hello from block 1, thread 1 (global thread 5)\n",
            "Hello from block 1, thread 2 (global thread 6)\n",
            "Hello from block 1, thread 3 (global thread 7)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}